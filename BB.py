# -*- coding: utf-8 -*-
"""ML-HW3.2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KUjHAhNJhtjdazqoemu11xfDmiiu_kpb
"""

import pandas as pd
import numpy as np
import math, time
import matplotlib.pyplot as plt

def read_input(filename):
  df = pd.read_csv(filename, header = None).to_numpy()
  y = df[:, [0]]*2-1
  X = df[:, range(1, df.shape[1])]
  return X, y

def average_predict(h_preds, alphas=1):
  sums = np.sum(h_preds*alphas, axis=1, keepdims=True)
  signs = np.where(sums > 0, 1, -1)
  return signs, sums


def get_accuracy(h_preds, y, alphas=1):
  signs, _ = average_predict(h_preds, alphas)
  return (np.sum((signs == y))/y.shape[0]*100)


def plot_accuracies(accuracies):
  plt.plot(accuracies[0], label = "train_accuracy")
  plt.plot(accuracies[1], label = "test_accuracy")
  plt.xlabel('round number')
  plt.ylabel('accuracy (%)')
  plt.legend()
  plt.show()


def plot_loss(losses):
  plt.plot(losses, label='loss value')
  plt.xlabel('iteration number')
  plt.legend()
  plt.show()

def three_node_traverse(x, tree):
  tree_type, features, labels = tree
  if tree_type == 0:
    if x[features[0]] == 0:
      if x[features[1]] == 0:
        return labels[0]
      else:
        return labels[1]
    else:
      if x[features[2]] == 0:
        return labels[2]
      else:
        return labels[3]
  elif tree_type == 1:
    if x[features[0]] == 0:
      return labels[0]
    else:
      if x[features[1]] == 0:
        return labels[1]
      else:
        if x[features[2]] == 0:
          return labels[2]
        else:
          return labels[3]
  elif tree_type == 2:
    if x[features[0]] == 0:
      return labels[0]
    else:
      if x[features[1]] == 1:
        return labels[1]
      else:
        if x[features[2]] == 0:
          return labels[2]
        else:
          return labels[3]
  elif tree_type == 3:
    if x[features[0]] == 1:
      return labels[0]
    else:
      if x[features[1]] == 1:
        return labels[1]
      else:
        if x[features[2]] == 0:
          return labels[2]
        else:
          return labels[3]
  elif tree_type == 4:
    if x[features[0]] == 1:
      return labels[0]
    else:
      if x[features[1]] == 0:
        return labels[1]
      else:
        if x[features[2]] == 0:
          return labels[2]
        else:
          return labels[3]


def three_node_tree_from_index(t, n):
  i = t
  tree_type, i = i//(n*n*n*16), i%(n*n*n*16)
  features = [0, 0, 0]
  features[0], i = i//(n*n*16), i%(n*n*16)
  features[1], i = i//(n*16), i%(n*16)
  features[2], i = i//16, i%16
  labels = [0, 0, 0, 0]
  labels[0], i = (i//8)*2-1, i%8
  labels[1], i = (i//4)*2-1, i%4
  labels[2], i = (i//2)*2-1, i%2
  labels[3] = i*2-1
  return tree_type, features, labels


def three_node_preds(X):
  m, n = X.shape
  H_len = n*n*n*5*16
  h_preds = np.zeros((m, H_len))
  for t in range(H_len):
    tree = three_node_tree_from_index(t, n)
    for i in range(m):
      h_preds[i, t] = three_node_traverse(X[i, :], tree)
  return h_preds


def print_three_node_tree(t, n):
  tree_type, features, labels = three_node_tree_from_index(t, n)
  print(f"\tfeature {features[0]}:")
  if tree_type == 0:
    print(f"\t\t0->feature {features[1]}:")
    print(f"\t\t\t0: label {'+' if labels[0] == 1 else '-'}")
    print(f"\t\t\t1: label {'+' if labels[1] == 1 else '-'}")
    print(f"\t\t1->feature {features[2]}:")
    print(f"\t\t\t0: label {'+' if labels[2] == 1 else '-'}")
    print(f"\t\t\t1: label {'+' if labels[3] == 1 else '-'}")
  elif tree_type == 1:
    print(f"\t\t0: label {'+' if labels[0] == 1 else '-'}")
    print(f"\t\t1->feature {features[1]}:")
    print(f"\t\t\t0: label {'+' if labels[1] == 1 else '-'}")
    print(f"\t\t\t1->feature {features[2]}:")
    print(f"\t\t\t\t0: label {'+' if labels[2] == 1 else '-'}")
    print(f"\t\t\t\t1: label {'+' if labels[3] == 1 else '-'}")
  elif tree_type == 2:
    print(f"\t\t0: label {'+' if labels[0] == 1 else '-'}")
    print(f"\t\t1->feature {features[1]}:")
    print(f"\t\t\t0->feature {features[2]}:")
    print(f"\t\t\t\t0: label {'+' if labels[2] == 1 else '-'}")
    print(f"\t\t\t\t1: label {'+' if labels[3] == 1 else '-'}")
    print(f"\t\t\t1: label {'+' if labels[1] == 1 else '-'}")
  elif tree_type == 3:
    print(f"\t\t0->feature {features[1]}:")
    print(f"\t\t\t0->feature {features[2]}:")
    print(f"\t\t\t\t0: label {'+' if labels[2] == 1 else '-'}")
    print(f"\t\t\t\t1: label {'+' if labels[3] == 1 else '-'}")
    print(f"\t\t\t1: label {'+' if labels[1] == 1 else '-'}")
    print(f"\t\t1: label {'+' if labels[0] == 1 else '-'}")
  elif tree_type == 4:
    print(f"\t\t0->feature {features[1]}:")
    print(f"\t\t\t0: label {'+' if labels[1] == 1 else '-'}")
    print(f"\t\t\t1->feature {features[2]}:")
    print(f"\t\t\t\t0: label {'+' if labels[2] == 1 else '-'}")
    print(f"\t\t\t\t1: label {'+' if labels[3] == 1 else '-'}")
    print(f"\t\t1: label {'+' if labels[0] == 1 else '-'}")
  print()

def single_node_preds(X):
  m, n = X.shape
  y = X*2-1
  return np.concatenate((y, -y, np.ones((m, n)), -np.ones((m, n))), axis=1)

def h_argmin_error(h_preds, y_train, w):
  errors = np.sum((h_preds != y_train)*w, axis = 0)
  h = np.argmin(errors)
  e = errors[h]
  return h, e


def ada_boost(X_train, y_train, X_test, y_test, T,
              h_preds_train, h_preds_test):
  print("\n######      ADA BOOST     ######")
  print(f"For train data shape: {X_train.shape} with {T} num of rounds:\n")
  m, n = X_train.shape
  w = np.ones((m,1))/m
  alphas, selected_hs = [], []
  accuracies = [[], []]
  for t in range(T):
    h, e = h_argmin_error(h_preds_train, y_train, w)
    alpha = 0.5*math.log((1-e)/e)
    alphas.append(alpha)
    selected_hs.append(h)
    numer = w*np.exp((-1)*y_train*h_preds_train[:, [h]]*alpha)
    denom = 2*math.sqrt(e*(1-e))
    w = numer/denom
    train_accuracy = get_accuracy(h_preds_train[:, selected_hs], y_train, alphas)
    test_accuracy = get_accuracy(h_preds_test[:, selected_hs], y_test, alphas)
    accuracies[0].append(train_accuracy)
    accuracies[1].append(test_accuracy)
    print(f"For round {t:>3d}-> selected h: {h}, e: {e:.2f},  alpha: {alpha:.2f},",
      f"train_acc: {train_accuracy:6.2f}%, test_acc: {test_accuracy:6.2f}%")
    if t < 5:
      print_three_node_tree(h, n)
  plot_accuracies(accuracies)

def coordinate_descent_loss(X_train, y_train, alphas, preds):
  _, sums = average_predict(preds, alphas)
  loss = np.sum(np.exp((-1)*y_train*sums))
  return loss


def coordinate_descent(X_train, y_train, X_test, y_test,
                       h_preds_train, h_preds_test):
  print("\n###### COORDINATE DESCENT ######\n")
  T = h_preds_train.shape[1]
  alphas = np.ones(T)
  counter, losses = 0, []
  while True:
    train_loss = coordinate_descent_loss(X_train, y_train, alphas, h_preds_train)
    losses.append(train_loss)
    t = counter%T
    _, pred_sums = average_predict(h_preds_train, alphas)
    avg_penalty = np.exp((-1)*y_train*(pred_sums-(h_preds_train[:,[t]]*alphas[t])))
    numer = np.sum((h_preds_train[:,[t]] == y_train)*avg_penalty)
    denom = np.sum((h_preds_train[:,[t]] != y_train)*avg_penalty)
    alphas[t] = 0.5*np.log(numer/denom)
    train_accuracy = get_accuracy(h_preds_train, y_train, alphas)
    counter += 1
    if train_accuracy > 85:
      break
  plot_loss(losses)
  print(f"\nfinal train_loss after {counter} iteration: {train_loss:.2f}")
  train_accuracy = get_accuracy(h_preds_train, y_train, alphas)
  test_accuracy = get_accuracy(h_preds_test, y_test, alphas)
  print(f"train acc: {train_accuracy:.2f}")
  print(f"test  acc: {test_accuracy:.2f}")
  print("alpha values:\n", alphas.reshape(22, -1))

def bagging(X_train, y_train, X_test, y_test, T,
            h_preds_train, h_preds_test):
  print("\n######       BAGGING      ######")
  print(f"For train data shape: {X_train.shape} with {T} num of bootstraps:\n\n")
  m, n = X_train.shape
  classifiers = []
  for t in range(T):
    selected_indices = np.random.choice(m, m)
    selected_h_preds = h_preds_train[selected_indices, :]
    selected_y_train = y_train[selected_indices, :]
    errors = np.sum((selected_h_preds != selected_y_train), axis = 0)
    selected_classifer = np.argmin(errors)
    classifiers.append(selected_classifer)
  train_accuracy = get_accuracy(h_preds_train[:, classifiers], y_train)
  test_accuracy = get_accuracy(h_preds_test[:, classifiers], y_test)
  print(f"train acc: {train_accuracy:.2f}")
  print(f"test  acc: {test_accuracy:.2f}")

def main():
  X_train, y_train = read_input("heart_train.data")
  X_test, y_test = read_input("heart_test.data")

  print("Question 2.1:")
  print("Applying three-node weak classifiers on training data... ",)
  start = time.time()
  three_node_h_preds_train = three_node_preds(X_train)
  print(f"done in {time.time()-start:.2f} seconds!")
  print("Applying three-node weak classifiers on test data...",)
  start = time.time()
  three_node_h_preds_test = three_node_preds(X_test)
  print(f"done in {time.time()-start:.2f} seconds!")

  ada_boost(X_train, y_train, X_test, y_test, 10,
            three_node_h_preds_train, three_node_h_preds_test)
  print("\n\n")

  print("Question 2.2:")
  print("Applying single-node weak classifiers on training data...", end='')
  single_node_h_preds_train = single_node_preds(X_train)
  print("Applying single-node weak classifiers on test data...", end='')
  single_node_h_preds_test = single_node_preds(X_test)
  coordinate_descent(X_train, y_train, X_test, y_test,
                     single_node_h_preds_train, single_node_h_preds_test)
  ada_boost(X_train, y_train, X_test, y_test, 20,
            single_node_h_preds_train, single_node_h_preds_test)
  bagging(X_train, y_train, X_test, y_test, 20,
          single_node_h_preds_train, single_node_h_preds_test)

main()